# -*- coding: utf-8 -*-
"""MLT-Mini Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-WrZTlp7qGJ8ZHkVrdlLBztmpi37tpTF
"""

!pip install lime

!pip install -U scikit-learn scikeras tensorflow

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt
from sklearn.inspection import permutation_importance
from lime.lime_tabular import LimeTabularExplainer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import tensorflow as tf
tf.get_logger().setLevel('ERROR')
from scikeras.wrappers import KerasClassifier
# Load dataset
df = pd.read_csv('train_u6lujuX_CVtuZ9i.csv')  # from Kaggle
df.drop('Loan_ID', axis=1, inplace=True)
df.fillna(df.mode().iloc[0], inplace=True)
# Encode categorical variables
for col in df.select_dtypes(include='object').columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
# Features and target
X = df.drop('Loan_Status', axis=1)
y = df['Loan_Status']

# Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Build ANN
model = Sequential([
    Dense(16, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(8, activation='relu'),
    Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=50, validation_split=0.2, batch_size=16, verbose=0)

# Prediction
y_pred = (model.predict(X_test, verbose=0) > 0.5).astype(int)

# Evaluation
print("\nAccuracy:", round(accuracy_score(y_test, y_pred) * 100, 2), "%")
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Plot Accuracy Line Graph
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

print("\nGenerating SHAP explanations")
explainer = shap.Explainer(model, X_train, feature_names=X.columns.tolist())
shap_values = explainer(X_test[:10])  # Limit for performance

# Summary plot for first 10 samples
shap.summary_plot(shap_values, X_test[:10], feature_names=X.columns)

print("\nGenerating LIME explanations")

# Initialize LimeTabularExplainer
lime_explainer = LimeTabularExplainer(
    training_data=X_train,
    feature_names=X.columns.tolist(),
    class_names=['Rejected', 'Approved'],  # 0: N, 1: Y
    mode='classification',
    discretize_continuous=True
)

# Choose a test instance (e.g., first instance)
i = 0
exp = lime_explainer.explain_instance(
    data_row=X_test[i],
    predict_fn=lambda x: np.concatenate([(1 - model.predict(x)), model.predict(x)], axis=1),
    num_features=10
)

# Show explanation in notebook
exp.show_in_notebook(show_table=True)

# For demonstration, let's create a dummy dataset
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=1000, n_features=10, random_state=42)
X = pd.DataFrame(X, columns=[f"Feature_{i}" for i in range(X.shape[1])])

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Define Keras model builder
def create_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)),
        tf.keras.layers.Dense(8, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Wrap with scikeras
clf = KerasClassifier(model=create_model, epochs=10, batch_size=32, verbose=0)

# Fit the model
clf.fit(X_train, y_train)

# Evaluate
score = clf.score(X_test, y_test)
print("Baseline accuracy:", score)

# Permutation importance
results = permutation_importance(clf, X_test, y_test, scoring='accuracy', n_repeats=10, random_state=42)

# Plot
importances = results.importances_mean
indices = np.argsort(importances)[::-1]
features = X.columns

plt.figure(figsize=(10, 6))
plt.title("Permutation Feature Importance")
plt.bar(range(X.shape[1]), importances[indices], align="center")
plt.xticks(range(X.shape[1]), features[indices], rotation=90)
plt.tight_layout()
plt.show()